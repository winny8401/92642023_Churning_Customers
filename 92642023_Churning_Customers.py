# -*- coding: utf-8 -*-
"""Assignment 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b9EF7Z5Wz9QjLle8yMKaXGt7aYd3PfaE
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.base import BaseEstimator
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Input, Dense
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.base import BaseEstimator
from sklearn.metrics import make_scorer, accuracy_score
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

data =pd.read_csv('/content/drive/MyDrive/colab labs/CustomerChurn_dataset.csv')

data

# Display basic statistics of the dataset
print(data.describe())

# Display the columns
print("Columns in the dataset:")
print(data.columns)

# Display the first few rows of the dataset
print("\nFirst few rows of the dataset:")
print(data.head())

# Checking  for missing values
print(data.isnull().sum())

"""EDA Analysis"""

# Visualize the distribution of the target variable 'Churn'
plt.figure(figsize=(6, 4)) # the width size is 6 and the lenght 4
sns.countplot(x='Churn', data=data)
plt.title('Distribution of Churn')
plt.show()

# Convert 'Churn' to numerical values using label encoding
data['Churn'] = data['Churn'].map({'No': 0, 'Yes': 1})

# Exploring  the relationship between numerical variables and Churn using bar charts
numerical_columns = ['tenure', 'MonthlyCharges', 'TotalCharges']

for column in numerical_columns:
    plt.figure(figsize=(10, 6))

    # Drop rows with missing values in the numerical column
    data_filtered = data.dropna(subset=[column])

    sns.barplot(x='Churn', y=column, data=data_filtered, ci=None)
    plt.title(f'{column} by Churn')
    plt.xlabel('Churn')
    plt.ylabel(column)
    plt.show()

"""Plotting the Churn distribution to know how many customers churned 'Yes' and how many did not 'No'."""

# Explore the relationship between categorical variables and Churn
categorical_columns = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService',
                        'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',
                        'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',
                        'Contract', 'PaperlessBilling', 'PaymentMethod']

for column in categorical_columns:
    plt.figure(figsize=(10, 4))
    sns.countplot(x=column, hue='Churn', data=data)
    plt.title(f'Relationship between {column} and Churn')
    plt.show()

correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

# Example: Box plot for tenure and Churn
plt.figure(figsize=(10, 6))
sns.boxplot(x='Churn', y='tenure', data=data)
plt.title('Box Plot of Tenure by Churn')
plt.show()

data.head(5)

"""## Data Preprocessing"""

data.info()

data['TotalCharges'] = data['TotalCharges'].apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()

data['TotalCharges'].isnull().sum()

data['TotalCharges'] = data['TotalCharges'].fillna(data['TotalCharges'].median())

data['TotalCharges'].isnull().sum()

data.info()

selected_df = data.drop(['customerID'], axis = 1)

cat_cols = selected_df.select_dtypes(include=['object']).columns

cat_cols = cat_cols.tolist()
cat_cols

le = LabelEncoder()
for col in cat_cols:
    selected_df[col] = le.fit_transform(selected_df[col])

selected_df.head()

"""### Feature selction using Random Forest Classifier."""

X = selected_df.drop('Churn', axis=1)
y = selected_df['Churn']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rfc = RandomForestClassifier()

# Fit the classifier
rfc.fit(X, y)

# Get feature importances
feature_importances = rfc.feature_importances_

# Create a DataFrame to store feature names and their importances
df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance in descending order
df = df.sort_values(by='Importance', ascending=False)

# Plot feature importances
top_features = 15  # Change this value to plot a different number of top features
plt.figure(figsize=(12, 6))
plt.barh(df['Feature'][:top_features], df['Importance'][:top_features])
plt.xlabel('Level of Importance')
plt.title('Essential Features')
plt.show()
final_features=df['Feature'][:top_features].values

# Display the list of important features
print("\n\nRelevant features:")
print(final_features)

"""### Training"""

X = selected_df[final_features]
y = selected_df['Churn']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense


input_layer = Input(shape=(X_train.shape[1],))
dense_layer_1 = Dense(64, activation='relu')(input_layer)
dense_layer_2 = Dense(32, activation='relu')(dense_layer_1)
output_layer = Dense(1, activation='sigmoid')(dense_layer_2)

model = Model(inputs=input_layer, outputs=output_layer, name='model')

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=32,
    validation_data=(X_test, y_test),
    verbose=0
)

## Display the model summary
model.summary()

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')

## Evaluate the model
y_pred = model.predict(X_test)
y_pred_binary = (y_pred > 0.5).astype(int)

## Calculate accuracy and AUC score
accuracy = accuracy_score(y_test, y_pred_binary)
auc_score = roc_auc_score(y_test, y_pred)

print(f"\nAccuracy: {accuracy}")
print(f"\nAUC Score: {auc_score}")

from sklearn.metrics import auc,roc_curve
# Calculate the AUC

predictions = [np.round(value) for value in y_pred]

fpr, tpr, thresholds = roc_curve(y_test, predictions)
roc_auc = auc(fpr, tpr)

plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([-0.001, 1])
plt.ylim([0, 1.001])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show();

accuracy = accuracy_score(y_test, predictions)
cofidence_factor = 2.58 * np.sqrt( (accuracy * (1 - accuracy)) / y_test.shape[0])
cofidence_factor

from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_auc_score, accuracy_score
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense

# Function to create the Keras model
def create_model(units=64):
    input_layer = Input(shape=(X_train.shape[1],))
    dense_layer_1 = Dense(units, activation='relu')(input_layer)
    dense_layer_2 = Dense(32, activation='relu')(dense_layer_1)
    output_layer = Dense(1, activation='sigmoid')(dense_layer_2)

    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Create a KerasClassifier
keras_model = KerasClassifier(build_fn=create_model, epochs=5, batch_size=32, verbose=True)

# Define a smaller hyperparameter space for grid search
param_grid = {
    'batch_size': [32],
    'optimizer': ['adam'],
}

# Use GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(estimator=keras_model, param_grid=param_grid, scoring=['accuracy', 'roc_auc'], refit='roc_auc', cv=3, verbose=True)
grid_result = grid_search.fit(X_train, y_train, validation_data=(X_test, y_test), verbose=True)


# Get the best parameters
best_params = grid_result.best_params_
print("Best Parameters:", best_params)

# Get the best model
best_model = grid_result.best_estimator_
best_model_main = grid_result.best_estimator_.model

# Evaluate the best model on the test set
y_pred = best_model.predict(X_test)
test_auc = roc_auc_score(y_test, y_pred)
test_accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))

print(f'Test AUC: {test_auc}, Test Accuracy: {test_accuracy}')

from tensorflow.keras.models import load_model
from tensorflow.keras.models import Model

## Save the model
model = model.save('churning_data.h5')